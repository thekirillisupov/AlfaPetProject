{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d286c82",
   "metadata": {
    "papermill": {
     "duration": 0.011978,
     "end_time": "2022-08-01T12:09:15.473488",
     "exception": false,
     "start_time": "2022-08-01T12:09:15.461510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 0. Imports and requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1782f86e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:15.497758Z",
     "iopub.status.busy": "2022-08-01T12:09:15.496538Z",
     "iopub.status.idle": "2022-08-01T12:09:18.219593Z",
     "shell.execute_reply": "2022-08-01T12:09:18.218554Z"
    },
    "papermill": {
     "duration": 2.73794,
     "end_time": "2022-08-01T12:09:18.222276",
     "exception": false,
     "start_time": "2022-08-01T12:09:15.484336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c14b7fb",
   "metadata": {
    "papermill": {
     "duration": 0.01038,
     "end_time": "2022-08-01T12:09:18.244008",
     "exception": false,
     "start_time": "2022-08-01T12:09:18.233628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b89b3bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:18.267105Z",
     "iopub.status.busy": "2022-08-01T12:09:18.266082Z",
     "iopub.status.idle": "2022-08-01T12:09:18.306669Z",
     "shell.execute_reply": "2022-08-01T12:09:18.305759Z"
    },
    "papermill": {
     "duration": 0.05436,
     "end_time": "2022-08-01T12:09:18.308871",
     "exception": false,
     "start_time": "2022-08-01T12:09:18.254511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_TRANSACTIONS_PATH = '../input/alfabattle2-sandbox/alfabattle2_sand_alfabattle2_train_transactions_contest/train_transactions_contest'\n",
    "TEST_TRANSACTIONS_PATH = '../input/alfabattle2-sandbox/alfabattle2_sand_alfabattle2_test_transactions_contest/test_transactions_contest'\n",
    "\n",
    "TRAIN_TARGET_PATH = '../input/alfabattle2-sandbox/alfabattle2_sand_alfabattle2_train_target.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1abaf60a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:18.331413Z",
     "iopub.status.busy": "2022-08-01T12:09:18.331112Z",
     "iopub.status.idle": "2022-08-01T12:09:18.689165Z",
     "shell.execute_reply": "2022-08-01T12:09:18.688210Z"
    },
    "papermill": {
     "duration": 0.37204,
     "end_time": "2022-08-01T12:09:18.691725",
     "exception": false,
     "start_time": "2022-08-01T12:09:18.319685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>product</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app_id  product  flag\n",
       "0       0        3     0\n",
       "1       1        1     0\n",
       "2       2        1     0\n",
       "3       3        1     0\n",
       "4       4        1     0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_frame = pd.read_csv(TRAIN_TARGET_PATH)\n",
    "target_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b580769b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:18.715525Z",
     "iopub.status.busy": "2022-08-01T12:09:18.715208Z",
     "iopub.status.idle": "2022-08-01T12:09:18.760204Z",
     "shell.execute_reply": "2022-08-01T12:09:18.759243Z"
    },
    "papermill": {
     "duration": 0.05918,
     "end_time": "2022-08-01T12:09:18.762513",
     "exception": false,
     "start_time": "2022-08-01T12:09:18.703333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_parquet_dataset_from_local(path_to_dataset: str, start_from: int = 0,\n",
    "                                     num_parts_to_read: int = 2, columns=None, verbose=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    читает num_parts_to_read партиций, преобразует их к pd.DataFrame и возвращает\n",
    "    :param path_to_dataset: путь до директории с партициями\n",
    "    :param start_from: номер партиции, с которой начать чтение\n",
    "    :param num_parts_to_read: количество партиций, которые требуется прочитать\n",
    "    :param columns: список колонок, которые нужно прочитать из партиции\n",
    "    :return: pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    res = []\n",
    "    dataset_paths = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset) \n",
    "                              if filename.startswith('part')])\n",
    "    \n",
    "    start_from = max(0, start_from)\n",
    "    chunks = dataset_paths[start_from: start_from + num_parts_to_read]\n",
    "    if verbose:\n",
    "        print('Reading chunks:\\n')\n",
    "        for chunk in chunks:\n",
    "            print(chunk)\n",
    "    for chunk_path in tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n",
    "        chunk = pd.read_parquet(chunk_path,columns=columns)\n",
    "        res.append(chunk)\n",
    "    return pd.concat(res).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec925470",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:18.787690Z",
     "iopub.status.busy": "2022-08-01T12:09:18.786199Z",
     "iopub.status.idle": "2022-08-01T12:09:18.832812Z",
     "shell.execute_reply": "2022-08-01T12:09:18.831955Z"
    },
    "papermill": {
     "duration": 0.061656,
     "end_time": "2022-08-01T12:09:18.835102",
     "exception": false,
     "start_time": "2022-08-01T12:09:18.773446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "features = ['currency', 'operation_kind', 'card_type', 'operation_type', 'operation_type_group', 'ecommerce_flag',\n",
    "            'payment_system', 'income_flag', 'mcc', 'country', 'city', 'mcc_category', 'day_of_week',\n",
    "            'hour', 'weekofyear', 'amnt', 'days_before', 'hour_diff']\n",
    "\n",
    "def pad_sequence(array, max_len) -> np.array:\n",
    "    \"\"\"\n",
    "    принимает список списков (array) и делает padding каждого вложенного списка до max_len\n",
    "    :param array: список списков\n",
    "    :param max_len: максимальная длина до которой нужно сделать padding\n",
    "    :return: np.array после padding каждого вложенного списка до одинаковой длины\n",
    "    \"\"\"\n",
    "    add_zeros = max_len - len(array[0])\n",
    "    return np.array([list(x) + [0] * add_zeros for x in array])\n",
    "\n",
    "\n",
    "def truncate(x, num_last_transactions=750):\n",
    "    return x.values.transpose()[:, -num_last_transactions:].tolist()\n",
    "\n",
    "\n",
    "def transform_transactions_to_sequences(transactions_frame: pd.DataFrame,\n",
    "                                        num_last_transactions=750) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    принимает frame с транзакциями клиентов, сортирует транзакции по клиентам\n",
    "    (внутри клиента сортирует транзакции по возрастанию), берет num_last_transactions танзакций,\n",
    "    возвращает новый pd.DataFrame с двумя колонками: app_id и sequences.\n",
    "    каждое значение в колонке sequences - это список списков.\n",
    "    каждый список - значение одного конкретного признака во всех клиентских транзакциях.\n",
    "    Всего признаков len(features), поэтому будет len(features) списков.\n",
    "    Данная функция крайне полезна для подготовки датасета для работы с нейронными сетями.\n",
    "    :param transactions_frame: фрейм с транзакциями клиентов\n",
    "    :param num_last_transactions: количество транзакций клиента, которые будут рассмотрены\n",
    "    :return: pd.DataFrame из двух колонок (app_id, sequences)\n",
    "    \"\"\"\n",
    "    return transactions_frame \\\n",
    "        .sort_values(['app_id', 'transaction_number']) \\\n",
    "        .groupby(['app_id'])[features] \\\n",
    "        .apply(lambda x: truncate(x, num_last_transactions=num_last_transactions)) \\\n",
    "        .reset_index().rename(columns={0: 'sequences'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456d47c",
   "metadata": {
    "papermill": {
     "duration": 0.010555,
     "end_time": "2022-08-01T12:09:18.858088",
     "exception": false,
     "start_time": "2022-08-01T12:09:18.847533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create padded buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee062c5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:18.881096Z",
     "iopub.status.busy": "2022-08-01T12:09:18.880795Z",
     "iopub.status.idle": "2022-08-01T12:09:18.927284Z",
     "shell.execute_reply": "2022-08-01T12:09:18.926456Z"
    },
    "papermill": {
     "duration": 0.060789,
     "end_time": "2022-08-01T12:09:18.929822",
     "exception": false,
     "start_time": "2022-08-01T12:09:18.869033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_padded_buckets(frame_of_sequences: pd.DataFrame, bucket_info: Dict[int, int],\n",
    "                          save_to_file_path=None, has_target=True):\n",
    "    \"\"\"\n",
    "    Функция реализует sequence_bucketing технику для обучения нейронных сетей.\n",
    "    Принимает на вход frame_of_sequences (результат работы функции transform_transactions_to_sequences),\n",
    "    словарь bucket_info, где для последовательности каждой длины указано, до какой максимальной длины нужно делать\n",
    "    padding, далее группирует транзакции по бакетам (на основе длины), делает padding транзакций и сохраняет результат\n",
    "    в pickle файл, если нужно\n",
    "    :param frame_of_sequences: pd.DataFrame c транзакциями (результат применения transform_transactions_to_sequences)\n",
    "    :param bucket_info: словарь, где для последовательности каждой длины указано, до какой максимальной длины нужно делать\n",
    "    padding\n",
    "    :param save_to_file_path: опциональный путь до файла, куда нужно сохранить результат\n",
    "    :param has_target: флаг, есть ли в frame_of_sequences целевая переменная или нет. Если есть, то\n",
    "    будет записано в результат\n",
    "    :return: возвращает словарь с следюущими ключами (padded_sequences, targets, app_id, products)\n",
    "    \"\"\"\n",
    "    frame_of_sequences['bucket_idx'] = frame_of_sequences.sequence_length.map(bucket_info)\n",
    "    padded_seq = []\n",
    "    targets = []\n",
    "    app_ids = []\n",
    "    products = []\n",
    "\n",
    "    for size, bucket in frame_of_sequences.groupby('bucket_idx'):\n",
    "        padded_sequences = bucket.sequences.apply(lambda x: pad_sequence(x, size)).values\n",
    "        padded_sequences = np.array([np.array(x) for x in padded_sequences])\n",
    "        padded_seq.append(padded_sequences)\n",
    "\n",
    "        if has_target:\n",
    "            targets.append(bucket.flag.values)\n",
    "\n",
    "        app_ids.append(bucket.app_id.values)\n",
    "        products.append(bucket['product'].values)\n",
    "\n",
    "    frame_of_sequences.drop(columns=['bucket_idx'], inplace=True)\n",
    "\n",
    "    dict_result = {\n",
    "        'padded_sequences': np.array(padded_seq, dtype=object),\n",
    "        'targets': np.array(targets, dtype=object) if targets else [],\n",
    "        'app_id': np.array(app_ids, dtype=object),\n",
    "        'products': np.array(products, dtype=object),\n",
    "    }\n",
    "\n",
    "    if save_to_file_path:\n",
    "        with open(save_to_file_path, 'wb') as f:\n",
    "            pickle.dump(dict_result, f)\n",
    "    return dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2dc9e4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:18.952670Z",
     "iopub.status.busy": "2022-08-01T12:09:18.952393Z",
     "iopub.status.idle": "2022-08-01T12:09:19.006082Z",
     "shell.execute_reply": "2022-08-01T12:09:19.005229Z"
    },
    "papermill": {
     "duration": 0.06742,
     "end_time": "2022-08-01T12:09:19.008055",
     "exception": false,
     "start_time": "2022-08-01T12:09:18.940635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../input/alfabattle2-sandbox/constants_for_rnn/constants_for_rnn/buckets_info.pkl', 'rb') as f:\n",
    "    mapping_seq_len_to_padded_len = pickle.load(f)\n",
    "    \n",
    "with open('../input/alfabattle2-sandbox/constants_for_rnn/constants_for_rnn/dense_features_buckets.pkl', 'rb') as f:\n",
    "    dense_features_buckets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9907ed41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:19.032192Z",
     "iopub.status.busy": "2022-08-01T12:09:19.031564Z",
     "iopub.status.idle": "2022-08-01T12:09:19.077716Z",
     "shell.execute_reply": "2022-08-01T12:09:19.076866Z"
    },
    "papermill": {
     "duration": 0.060581,
     "end_time": "2022-08-01T12:09:19.079721",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.019140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_buckets_from_transactions(path_to_dataset, save_to_path, frame_with_ids = None, \n",
    "                                     num_parts_to_preprocess_at_once: int = 1, \n",
    "                                     num_parts_total=50, has_target=False):\n",
    "    \"\"\"\n",
    "    Функция `create_buckets_from_transactions` ниже реализует следующий набор действий:\n",
    "    * Читает `num_parts_to_preprocess_at_once` частей датасета в память\n",
    "    * Преобразует вещественные и численные признаки к категориальным (используя `np.digitize` и подготовленные бины)\n",
    "    * Формирует фрейм с транзакциями в виде последовательностей с помощью `transform_transactions_to_sequences`.\n",
    "    * Если указан `frame_with_ids`, то использует `app_id` из `frame_with_ids` - актуально, чтобы выделить валидационную выборку.\n",
    "    * Реализует технику `sequence_bucketing` и сохраняет словарь обработанных последовательностей в `.pkl` файл\n",
    "    \"\"\"\n",
    "    block = 0\n",
    "    for step in tqdm(range(0, num_parts_total, num_parts_to_preprocess_at_once), \n",
    "                                   desc=\"Transforming transactions data\"):\n",
    "        transactions_frame = read_parquet_dataset_from_local(path_to_dataset, step, num_parts_to_preprocess_at_once, \n",
    "                                                             verbose=True)\n",
    "        for dense_col in ['amnt', 'days_before', 'hour_diff']:\n",
    "            transactions_frame[dense_col] = np.digitize(transactions_frame[dense_col], bins=dense_features_buckets[dense_col])\n",
    "            \n",
    "        seq = transform_transactions_to_sequences(transactions_frame)\n",
    "        seq['sequence_length'] = seq.sequences.apply(lambda x: len(x[1]))\n",
    "        \n",
    "        if frame_with_ids is not None:\n",
    "            seq = seq.merge(frame_with_ids, on='app_id')\n",
    "\n",
    "        block_as_str = str(block)\n",
    "        if len(block_as_str) == 1:\n",
    "            block_as_str = '00' + block_as_str\n",
    "        else:\n",
    "            block_as_str = '0' + block_as_str\n",
    "            how\n",
    "        processed_fragment =  create_padded_buckets(seq, mapping_seq_len_to_padded_len, has_target=has_target, \n",
    "                                                    save_to_file_path=os.path.join(save_to_path, \n",
    "                                                                                   f'processed_chunk_{block_as_str}.pkl'))\n",
    "        block += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9bc1e",
   "metadata": {
    "papermill": {
     "duration": 0.010462,
     "end_time": "2022-08-01T12:09:19.101047",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.090585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* Разобьем имеющиеся данные на `train` и `val` части. Воспользуемся самым простым способом - для валидации используем 10% случайных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17901987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:19.124170Z",
     "iopub.status.busy": "2022-08-01T12:09:19.123324Z",
     "iopub.status.idle": "2022-08-01T12:09:19.177592Z",
     "shell.execute_reply": "2022-08-01T12:09:19.176569Z"
    },
    "papermill": {
     "duration": 0.068216,
     "end_time": "2022-08-01T12:09:19.179971",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.111755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/train-val-buckets/val_buckets/processed_chunk_000.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_001.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_002.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_003.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_004.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_005.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_006.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_007.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_008.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_009.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_010.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_011.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_012.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_013.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_014.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_015.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_016.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_017.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_018.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_019.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_020.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_021.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_022.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_023.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_024.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_025.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_026.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_027.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_028.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_029.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_030.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_031.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_032.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_033.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_034.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_035.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_036.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_037.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_038.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_039.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_040.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_041.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_042.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_043.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_044.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_045.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_046.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_047.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_048.pkl',\n",
       " '../input/train-val-buckets/val_buckets/processed_chunk_049.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_dataset = '../input/train-val-buckets/val_buckets'\n",
    "dir_with_datasets = os.listdir(path_to_dataset)\n",
    "dataset_val = sorted([os.path.join(path_to_dataset, x) for x in dir_with_datasets])\n",
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7678cba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:19.203467Z",
     "iopub.status.busy": "2022-08-01T12:09:19.202686Z",
     "iopub.status.idle": "2022-08-01T12:09:19.254340Z",
     "shell.execute_reply": "2022-08-01T12:09:19.253416Z"
    },
    "papermill": {
     "duration": 0.065525,
     "end_time": "2022-08-01T12:09:19.256485",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.190960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/train-val-buckets/train_buckets/processed_chunk_000.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_001.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_002.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_003.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_004.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_005.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_006.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_007.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_008.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_009.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_010.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_011.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_012.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_013.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_014.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_015.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_016.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_017.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_018.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_019.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_020.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_021.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_022.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_023.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_024.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_025.pkl',\n",
       " '../input/train-val-buckets/train_buckets/processed_chunk_026.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_dataset = '../input/train-val-buckets/train_buckets'\n",
    "dir_with_datasets = os.listdir(path_to_dataset)\n",
    "dataset_train = sorted([os.path.join(path_to_dataset, x) for x in dir_with_datasets])\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79b96d",
   "metadata": {
    "papermill": {
     "duration": 0.010915,
     "end_time": "2022-08-01T12:09:19.278568",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.267653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d85e404",
   "metadata": {
    "papermill": {
     "duration": 0.010913,
     "end_time": "2022-08-01T12:09:19.300591",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.289678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    * `data_generators.batches_generator` - функция-генератор, итеративно возвращает батчи, поддерживает батчи для `tensorflow.keras` и `torch.nn.module` моделей. В зависимости от флага `is_train` может быть использована для генерации батчей на train/val/test стадию.\n",
    "    * функция `pytorch_training.train_epoch` - обучает модель одну эпоху.\n",
    "    * функция `pytorch_training.eval_model` - проверяет качество модели на отложенной выборке и возвращает roc_auc_score.\n",
    "    * функция `pytorch_training.inference` - делает предикты на новых данных и готовит фрейм для проверяющей системы.\n",
    "    * класс `training_aux.EarlyStopping` - реализует early_stopping, сохраняя лучшую модель. Пример использования приведен ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33bd3011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:19.324788Z",
     "iopub.status.busy": "2022-08-01T12:09:19.324497Z",
     "iopub.status.idle": "2022-08-01T12:09:19.424809Z",
     "shell.execute_reply": "2022-08-01T12:09:19.423828Z"
    },
    "papermill": {
     "duration": 0.115851,
     "end_time": "2022-08-01T12:09:19.427745",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.311894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8add304f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:19.452134Z",
     "iopub.status.busy": "2022-08-01T12:09:19.451834Z",
     "iopub.status.idle": "2022-08-01T12:09:19.508259Z",
     "shell.execute_reply": "2022-08-01T12:09:19.507392Z"
    },
    "papermill": {
     "duration": 0.071121,
     "end_time": "2022-08-01T12:09:19.510566",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.439445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transaction_features = ['currency', 'operation_kind', 'card_type', 'operation_type',\n",
    "                        'operation_type_group', 'ecommerce_flag', 'payment_system',\n",
    "                        'income_flag', 'mcc', 'country', 'city', 'mcc_category',\n",
    "                        'day_of_week', 'hour', 'weekofyear', 'amnt', 'days_before', 'hour_diff']\n",
    "\n",
    "def batches_generator(list_of_paths, batch_size=32, shuffle=False, is_infinite=False,\n",
    "                      verbose=False, device=None, output_format='torch', is_train=True):\n",
    "    \"\"\"\n",
    "    функция для создания батчей на вход для нейронной сети для моделей на keras и pytorch.\n",
    "    так же может использоваться как функция на стадии инференса\n",
    "    :param list_of_paths: путь до директории с предобработанными последовательностями\n",
    "    :param batch_size: размер батча\n",
    "    :param shuffle: флаг, если True, то перемешивает list_of_paths и так же\n",
    "    перемешивает последовательности внутри файла\n",
    "    :param is_infinite: флаг, если True,  то создает бесконечный генератор батчей\n",
    "    :param verbose: флаг, если True, то печатает текущий обрабатываемый файл\n",
    "    :param device: device на который положить данные, если работа на торче\n",
    "    :param output_format: допустимые варианты ['tf', 'torch']. Если 'torch', то возвращает словарь,\n",
    "    где ключи - батчи из признаков, таргетов и app_id. Если 'tf', то возвращает картеж: лист input-ов\n",
    "    для модели, и список таргетов.\n",
    "    :param is_train: флаг, Если True, то для кераса вернет (X, y), где X - input-ы в модель, а y - таргеты, \n",
    "    если False, то в y будут app_id; для torch вернет словарь с ключами на device.\n",
    "    :return: бачт из последовательностей и таргетов (или app_id)\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(list_of_paths)\n",
    "\n",
    "        for path in list_of_paths:\n",
    "            if verbose:\n",
    "                print(f'reading {path}')\n",
    "\n",
    "            with open(path, 'rb') as f:\n",
    "                '''\n",
    "                26.pkl is truncated \n",
    "                '''\n",
    "                if path == '../input/train-val-buckets/train_buckets/processed_chunk_026.pkl':\n",
    "                    continue\n",
    "                data = pickle.load(f)\n",
    "            padded_sequences, targets, products = data['padded_sequences'], data['targets'], data[\n",
    "                'products']\n",
    "            app_ids = data['app_id']\n",
    "            indices = np.arange(len(products))\n",
    "\n",
    "            if shuffle:\n",
    "                np.random.shuffle(indices)\n",
    "                padded_sequences = padded_sequences[indices]\n",
    "                targets = targets[indices]\n",
    "                products = products[indices]\n",
    "                app_ids = app_ids[indices]\n",
    "\n",
    "            for idx in range(len(products)):\n",
    "                bucket, product = padded_sequences[idx], products[idx]\n",
    "                app_id = app_ids[idx]\n",
    "                \n",
    "                if is_train:\n",
    "                    target = targets[idx]\n",
    "                \n",
    "                for jdx in range(0, len(bucket), batch_size):\n",
    "                    batch_sequences = bucket[jdx: jdx + batch_size]\n",
    "                    if is_train:\n",
    "                        batch_targets = target[jdx: jdx + batch_size]\n",
    "                    \n",
    "                    batch_products = product[jdx: jdx + batch_size]\n",
    "                    batch_app_ids = app_id[jdx: jdx + batch_size]\n",
    "                    \n",
    "                    if output_format == 'tf':\n",
    "                        batch_sequences = [batch_sequences[:, i] for i in\n",
    "                                           range(len(transaction_features))]\n",
    "                        \n",
    "                        # append product as input to tf model\n",
    "                        batch_sequences.append(batch_products)\n",
    "                        if is_train:\n",
    "                            yield batch_sequences, batch_targets\n",
    "                        else:\n",
    "                             yield batch_sequences, batch_app_ids\n",
    "                    else:\n",
    "                        batch_sequences = [torch.LongTensor(batch_sequences[:, i]).to(device)\n",
    "                                           for i in range(len(transaction_features))]\n",
    "                        if is_train:\n",
    "                            yield dict(transactions_features=batch_sequences,\n",
    "                                       product=torch.LongTensor(batch_products).to(device),\n",
    "                                       label=torch.LongTensor(batch_targets).to(device),\n",
    "                                       app_id=batch_app_ids)\n",
    "                        else:\n",
    "                            yield dict(transactions_features=batch_sequences,\n",
    "                                       product=torch.LongTensor(batch_products).to(device),\n",
    "                                       app_id=batch_app_ids)\n",
    "        if not is_infinite:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16a22b11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:19.534823Z",
     "iopub.status.busy": "2022-08-01T12:09:19.534054Z",
     "iopub.status.idle": "2022-08-01T12:09:19.578802Z",
     "shell.execute_reply": "2022-08-01T12:09:19.577950Z"
    },
    "papermill": {
     "duration": 0.059061,
     "end_time": "2022-08-01T12:09:19.580873",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.521812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, dataset_train, batch_size=64, shuffle=True,\n",
    "                print_loss_every_n_batches=500, device=None):\n",
    "    \"\"\"\n",
    "    делает одну эпоху обучения модели, логирует\n",
    "    :param model: nn.Module модель\n",
    "    :param optimizer: nn.optim оптимизатор\n",
    "    :param dataset_train: путь до директории с последовательностями\n",
    "    :param batch_size: размерм батча\n",
    "    :param shuffle: флаг, если True, то перемешивает данные\n",
    "    :param print_loss_every_n_batches: число батчей после которых логируется лосс на этих батчах\n",
    "    :param device: device, на который будут положены данные внутри батча\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    train_generator = batches_generator(dataset_train, batch_size=batch_size, shuffle=shuffle,\n",
    "                                        device=device, is_train=True, output_format='torch')\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    num_batches = 1\n",
    "    running_loss = 0.0\n",
    "    max_grad_norm = 1000\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch in tqdm(train_generator, desc='Training'):\n",
    "\n",
    "        output = torch.flatten(model(batch['transactions_features'], batch['product']))\n",
    "\n",
    "        batch_loss = loss_function(output, batch['label'].float())\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += batch_loss\n",
    "\n",
    "        if num_batches % print_loss_every_n_batches == 0:\n",
    "            print(f'Training loss after {num_batches} batches: {running_loss / num_batches}', end='\\r')\n",
    "        \n",
    "        num_batches += 1\n",
    "    \n",
    "    print(f'Training loss after epoch: {running_loss / num_batches}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ef3cd01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:19.605492Z",
     "iopub.status.busy": "2022-08-01T12:09:19.605191Z",
     "iopub.status.idle": "2022-08-01T12:09:19.651128Z",
     "shell.execute_reply": "2022-08-01T12:09:19.650292Z"
    },
    "papermill": {
     "duration": 0.060675,
     "end_time": "2022-08-01T12:09:19.653234",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.592559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batches_complete_generator():\n",
    "    '''\n",
    "    reinitialize new generator which union incomplete batches and after split this into full batches\n",
    "    '''\n",
    "    tr_generator = batches_generator([dataset_train[0]], batch_size=128, shuffle=False,\n",
    "                        device=device, is_train=True, output_format='torch')\n",
    "    \n",
    "    incomplete_batches = {}\n",
    "    \n",
    "    for idx, batch in enumerate(tr_generator):\n",
    "        if batch['transactions_features'][0].shape[0] != batch_size:\n",
    "            incomplete_batches[batch['transactions_features'][0].shape[1]] = incomplete_batches.get(batch['transactions_features'][0].shape[1], []).append(batch)\n",
    "        else:\n",
    "            yield batch\n",
    "            \n",
    "    split_batches = {\n",
    "        'padded_sequences': [],\n",
    "        'targets': [],\n",
    "        'app_id': [],\n",
    "        'products': [],\n",
    "    }\n",
    "    \n",
    "    for key_bathes, val_batches in incomplete_batches.items():\n",
    "        for key in split_batches:\n",
    "            concatenate_batches = torch.cat(*(batch['padded_sequences'] for batch in val_batches), dim=0)\n",
    "            split_batches[key] = torch.split(concatenate_batches, split_size_or_sections=batch_size)\n",
    "            \n",
    "        for idx in range(len(split_batches['padded_sequences'])):\n",
    "            yield {'padded_sequences': split_batches['padded_sequences'][idx],\n",
    "                    'targets': split_batches['targets'][idx],\n",
    "                    'app_id': split_batches['app_id'][idx],\n",
    "                    'products': split_batches['products'][idx]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e87aee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:19.677168Z",
     "iopub.status.busy": "2022-08-01T12:09:19.676899Z",
     "iopub.status.idle": "2022-08-01T12:09:19.719361Z",
     "shell.execute_reply": "2022-08-01T12:09:19.718531Z"
    },
    "papermill": {
     "duration": 0.056877,
     "end_time": "2022-08-01T12:09:19.721466",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.664589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(model, dataset_val, batch_size=32, device=None) -> float:\n",
    "    \"\"\"\n",
    "    функция для оценки качества модели на отложенной выборке, возвращает roc-auc на валидационной\n",
    "    выборке\n",
    "    :param model: nn.Module модель\n",
    "    :param dataset_val: путь до директории с последовательностями\n",
    "    :param batch_size: размер батча\n",
    "    :param device: device, на который будут положены данные внутри батча\n",
    "    :return: val roc-auc score\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    targets = []\n",
    "    val_generator = batches_generator(dataset_val, batch_size=batch_size, shuffle=False,\n",
    "                                      device=device, is_train=True, output_format='torch')\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(val_generator, desc='Evaluating model'):\n",
    "        targets.extend(batch['label'].detach().cpu().numpy().flatten())\n",
    "        output = model(batch['transactions_features'], batch['product'])\n",
    "        preds.extend(output.detach().cpu().numpy().flatten())\n",
    "\n",
    "    return roc_auc_score(targets, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2b487cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:19.745763Z",
     "iopub.status.busy": "2022-08-01T12:09:19.745493Z",
     "iopub.status.idle": "2022-08-01T12:09:19.788961Z",
     "shell.execute_reply": "2022-08-01T12:09:19.788138Z"
    },
    "papermill": {
     "duration": 0.058216,
     "end_time": "2022-08-01T12:09:19.791023",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.732807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(model, dataset_test, batch_size=32, device=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    функция, которая делает предикты на новых данных, возвращает pd.DataFrame из двух колонок:\n",
    "    (app_id, score)\n",
    "    :param model: nn.Module модель\n",
    "    :param dataset_test: путь до директории с последовательностями\n",
    "    :param batch_size: размер батча\n",
    "    :param device: device, на который будут положены данные внутри батча\n",
    "    :return: pd.DataFrame из двух колонок: (app_id, score)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    app_ids = []\n",
    "    test_generator = batches_generator(dataset_test, batch_size=batch_size, shuffle=False,\n",
    "                                       verbose=False, device=device, is_train=False,\n",
    "                                       output_format='torch')\n",
    "\n",
    "    for batch in tqdm(test_generator, desc='Test time predictions'):\n",
    "        app_ids.extend(batch['app_id'])\n",
    "        output = model(batch['transactions_features'], batch['product'])\n",
    "        preds.extend(output.detach().cpu().numpy().flatten())\n",
    "        \n",
    "    return pd.DataFrame({\n",
    "        'app_id': app_ids,\n",
    "        'score': preds\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b57f397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:19.815223Z",
     "iopub.status.busy": "2022-08-01T12:09:19.814523Z",
     "iopub.status.idle": "2022-08-01T12:09:19.862720Z",
     "shell.execute_reply": "2022-08-01T12:09:19.861787Z"
    },
    "papermill": {
     "duration": 0.062497,
     "end_time": "2022-08-01T12:09:19.864765",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.802268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation metric doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(self, patience=7, mode='min', verbose=False, delta=0, save_path='checkpoint.hdf5', metric_name=None, save_format='torch'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            mode (str): One of ['min', 'max'], whether to maximize or minimaze the metric.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            save_path (str): Path to saved model\n",
    "        \"\"\"\n",
    "        if mode not in ['min', 'max']:\n",
    "            raise ValueError(f'Unrecognized mode: {mode}!')\n",
    "\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_prev_score = np.Inf if mode == 'min' else -np.Inf\n",
    "        self.delta = delta\n",
    "        self.save_path = save_path\n",
    "        self.metric_name = 'metric' if not metric_name else metric_name\n",
    "        if save_format not in ['torch', 'tf']:\n",
    "            raise ValueError('Expected to save in one of the following formats: [\"torch\", \"tf\"]')\n",
    "        self.save_format = save_format\n",
    "        \n",
    "    def __call__(self, metric_value, model):\n",
    "\n",
    "        score = -metric_value if self.mode == 'min' else metric_value\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(metric_value, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(\n",
    "                f'No imporvement in Validation {self.metric_name}. Current: {score:.6f}. Current best: {self.best_score:.6f}')\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(metric_value, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, metric_value, model):\n",
    "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f'Validation {self.metric_name} improved ({self.best_prev_score:.6f} --> {metric_value:.6f}).  Saving model ...')\n",
    "        if self.save_format == 'tf':\n",
    "            model.save_weights(self.save_path)\n",
    "        else:\n",
    "            torch.save(model.state_dict(), self.save_path)\n",
    "            \n",
    "        self.best_prev_score = metric_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b047341c",
   "metadata": {
    "papermill": {
     "duration": 0.010853,
     "end_time": "2022-08-01T12:09:19.887118",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.876265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* Все признаки в нашей модели будут категориальными. Для их представления в модели используем категориальные эмбеддинги. Для этого нужно каждому категориальному признаку задать размерность латентного пространства. Используем [формулу](https://forums.fast.ai/t/size-of-embedding-for-categorical-variables/42608) из библиотеки `fast.ai`. Все отображения хранятся в файле `embedding_projections.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c424c806",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:19.911530Z",
     "iopub.status.busy": "2022-08-01T12:09:19.910754Z",
     "iopub.status.idle": "2022-08-01T12:09:19.958836Z",
     "shell.execute_reply": "2022-08-01T12:09:19.957996Z"
    },
    "papermill": {
     "duration": 0.062681,
     "end_time": "2022-08-01T12:09:19.960886",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.898205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../input/alfabattle2-sandbox/constants_for_rnn/constants_for_rnn/embedding_projections.pkl', 'rb') as f:\n",
    "    embedding_projections = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2584d644",
   "metadata": {
    "papermill": {
     "duration": 0.010938,
     "end_time": "2022-08-01T12:09:19.983192",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.972254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* Реализуем модель. Все входные признаки представим в виде эмбеддингов, сконкатенируем, чтобы получить векторное представление транзакции. Подадим последовательности в `GRU` рекуррентную сеть. Используем последнее скрытое состояние в качестве выхода сети. Представим признак `product` в виде отдельного эмбеддинга. Сконкатенируем его с выходом сети. На основе такого входа построим небольшой `MLP`, выступающий классификатором для целевой задачи. Используем градиентный спуск, чтобы решить оптимизационную задачу. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc6885a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:20.007714Z",
     "iopub.status.busy": "2022-08-01T12:09:20.007233Z",
     "iopub.status.idle": "2022-08-01T12:09:20.055522Z",
     "shell.execute_reply": "2022-08-01T12:09:20.054694Z"
    },
    "papermill": {
     "duration": 0.062825,
     "end_time": "2022-08-01T12:09:20.057626",
     "exception": false,
     "start_time": "2022-08-01T12:09:19.994801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransactionsRnn(nn.Module):\n",
    "    def __init__(self, transactions_cat_features, embedding_projections, product_col_name='product', rnn_units=128, top_classifier_units=32):\n",
    "        super(TransactionsRnn, self).__init__()\n",
    "        self._transaction_cat_embeddings = nn.ModuleList([self._create_embedding_projection(*embedding_projections[feature]) \n",
    "                                                          for feature in transactions_cat_features])\n",
    "                \n",
    "        self._product_embedding = self._create_embedding_projection(*embedding_projections[product_col_name], padding_idx=None)\n",
    "        \n",
    "        self._gru = nn.GRU(input_size=sum([embedding_projections[x][1] for x in transactions_cat_features]),\n",
    "                           hidden_size=rnn_units, \n",
    "                           batch_first=True, \n",
    "                           bidirectional=False)\n",
    "        \n",
    "        self._hidden_size = rnn_units\n",
    "        \n",
    "        self._top_classifier = nn.Linear(in_features=rnn_units+embedding_projections[product_col_name][1], \n",
    "                                         out_features=top_classifier_units)\n",
    "        self._intermediate_activation = nn.ReLU()\n",
    "        \n",
    "        self._head = nn.Linear(in_features=top_classifier_units, out_features=1)\n",
    "    \n",
    "    def forward(self, transactions_cat_features, product_feature):\n",
    "        batch_size = product_feature.shape[0]\n",
    "        \n",
    "        embeddings = [embedding(transactions_cat_features[i]) for i, embedding in enumerate(self._transaction_cat_embeddings)]\n",
    "        \n",
    "        concated_embeddings = torch.cat(embeddings, dim=-1)\n",
    "        \n",
    "        _, last_hidden = self._gru(concated_embeddings)\n",
    "        \n",
    "        last_hidden = torch.reshape(last_hidden.permute(1, 2, 0), shape=(batch_size, self._hidden_size))\n",
    "        \n",
    "        product_embed = self._product_embedding(product_feature)\n",
    "\n",
    "        intermediate_concat = torch.cat([last_hidden, product_embed], dim=-1)\n",
    "  \n",
    "        classification_hidden = self._top_classifier(intermediate_concat)\n",
    "\n",
    "        activation = self._intermediate_activation(classification_hidden)\n",
    "        \n",
    "        logit = self._head(activation)\n",
    "        \n",
    "        return logit\n",
    "    \n",
    "    @classmethod\n",
    "    def _create_embedding_projection(cls, cardinality, embed_size, add_missing=True, padding_idx=0):\n",
    "        add_missing = 1 if add_missing else 0\n",
    "        return nn.Embedding(num_embeddings=cardinality+add_missing, embedding_dim=embed_size, padding_idx=padding_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b686b455",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:20.081318Z",
     "iopub.status.busy": "2022-08-01T12:09:20.081048Z",
     "iopub.status.idle": "2022-08-01T12:09:20.124277Z",
     "shell.execute_reply": "2022-08-01T12:09:20.123454Z"
    },
    "papermill": {
     "duration": 0.05748,
     "end_time": "2022-08-01T12:09:20.126222",
     "exception": false,
     "start_time": "2022-08-01T12:09:20.068742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiGRU2FCL(TransactionsRnn):\n",
    "    def __init__(self, transactions_cat_features, embedding_projections, product_col_name='product', rnn_units=128, top_classifier_units=32):\n",
    "        super(BiGRU2FCL, self).__init__(transactions_cat_features, embedding_projections, product_col_name, rnn_units, top_classifier_units)\n",
    "\n",
    "        self._gru = nn.GRU(input_size=sum([embedding_projections[x][1] for x in transactions_cat_features]),\n",
    "                           hidden_size=rnn_units, \n",
    "                           batch_first=True, \n",
    "                           bidirectional=True)\n",
    "        \n",
    "        self._hidden_size = rnn_units*2\n",
    "        \n",
    "        self._top_classifier = nn.Linear(in_features=rnn_units*2+embedding_projections[product_col_name][1], \n",
    "                                         out_features=top_classifier_units*2)\n",
    "        \n",
    "        self._head = nn.Linear(in_features=top_classifier_units*2, out_features=1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2762f29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:20.150131Z",
     "iopub.status.busy": "2022-08-01T12:09:20.149409Z",
     "iopub.status.idle": "2022-08-01T12:09:20.193442Z",
     "shell.execute_reply": "2022-08-01T12:09:20.192606Z"
    },
    "papermill": {
     "duration": 0.058133,
     "end_time": "2022-08-01T12:09:20.195602",
     "exception": false,
     "start_time": "2022-08-01T12:09:20.137469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiGRU2FCL_Norm(BiGRU2FCL):\n",
    "    '''\n",
    "    add BatchNorm layer before FCL\n",
    "    '''\n",
    "    def __init__(self, transactions_cat_features, embedding_projections, product_col_name='product', rnn_units=128, top_classifier_units=32):\n",
    "        super(BiGRU2FCL_Norm, self).__init__(transactions_cat_features, embedding_projections, product_col_name, rnn_units, top_classifier_units)\n",
    "        self._batch_norm = nn.BatchNorm1d(num_features=self._hidden_size+embedding_projections[product_col_name][1])\n",
    "        \n",
    "    def forward(self, transactions_cat_features, product_feature):\n",
    "        batch_size = product_feature.shape[0]\n",
    "        \n",
    "        embeddings = [embedding(transactions_cat_features[i]) for i, embedding in enumerate(self._transaction_cat_embeddings)]\n",
    "        \n",
    "        concated_embeddings = torch.cat(embeddings, dim=-1)\n",
    "        \n",
    "        _, last_hidden = self._gru(concated_embeddings)\n",
    "        \n",
    "        last_hidden = torch.reshape(last_hidden.permute(1, 2, 0), shape=(batch_size, self._hidden_size))\n",
    "        \n",
    "        product_embed = self._product_embedding(product_feature)\n",
    "        \n",
    "        intermediate_concat = torch.cat([last_hidden, product_embed], dim=-1)\n",
    "        \n",
    "        norm_intermediate_concat = self._batch_norm(intermediate_concat)\n",
    "            \n",
    "        classification_hidden = self._top_classifier(norm_intermediate_concat)\n",
    "\n",
    "        activation = self._intermediate_activation(classification_hidden)\n",
    "        \n",
    "        logit = self._head(activation)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b69ce7c",
   "metadata": {
    "papermill": {
     "duration": 0.010823,
     "end_time": "2022-08-01T12:09:20.217801",
     "exception": false,
     "start_time": "2022-08-01T12:09:20.206978",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c19d7",
   "metadata": {
    "papermill": {
     "duration": 0.011107,
     "end_time": "2022-08-01T12:09:20.240267",
     "exception": false,
     "start_time": "2022-08-01T12:09:20.229160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Work Pipeline\n",
    "* RNN: variational DropOut\n",
    "* redesign batch generation\n",
    "* RNN regulirization: forget_gate ~ 1, gradient cliping\n",
    "* build CNN + LSTM + CRF model in tensorflow\n",
    "* analyse advanced baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2eb9a865",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:20.263922Z",
     "iopub.status.busy": "2022-08-01T12:09:20.263390Z",
     "iopub.status.idle": "2022-08-01T12:09:22.953744Z",
     "shell.execute_reply": "2022-08-01T12:09:22.952517Z"
    },
    "papermill": {
     "duration": 2.705021,
     "end_time": "2022-08-01T12:09:22.956421",
     "exception": false,
     "start_time": "2022-08-01T12:09:20.251400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './rnn_baseline/checkpoints/pytorch_baseline': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir ./rnn_baseline\n",
    "\n",
    "! mkdir ./rnn_baseline/checkpoints\n",
    "\n",
    "! rm -r ./rnn_baseline/checkpoints/pytorch_baseline\n",
    "! mkdir ./rnn_baseline/checkpoints/pytorch_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4baa3ed",
   "metadata": {
    "papermill": {
     "duration": 0.011144,
     "end_time": "2022-08-01T12:09:22.979825",
     "exception": false,
     "start_time": "2022-08-01T12:09:22.968681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* Для того, чтобы детектировать переобучение используем EarlyStopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d5519dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:23.004848Z",
     "iopub.status.busy": "2022-08-01T12:09:23.003847Z",
     "iopub.status.idle": "2022-08-01T12:09:23.050024Z",
     "shell.execute_reply": "2022-08-01T12:09:23.049119Z"
    },
    "papermill": {
     "duration": 0.060958,
     "end_time": "2022-08-01T12:09:23.052098",
     "exception": false,
     "start_time": "2022-08-01T12:09:22.991140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_checkpoints = './rnn_baseline/checkpoints/pytorch_baseline/'\n",
    "es = EarlyStopping(patience=3, mode='max', verbose=True, save_path=os.path.join(path_to_checkpoints, 'best_checkpoint.pt'), \n",
    "                   metric_name='ROC-AUC', save_format='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "917be438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:23.076784Z",
     "iopub.status.busy": "2022-08-01T12:09:23.075930Z",
     "iopub.status.idle": "2022-08-01T12:09:23.116131Z",
     "shell.execute_reply": "2022-08-01T12:09:23.115274Z"
    },
    "papermill": {
     "duration": 0.054509,
     "end_time": "2022-08-01T12:09:23.118179",
     "exception": false,
     "start_time": "2022-08-01T12:09:23.063670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "train_batch_size = 128\n",
    "val_batch_szie = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97191c35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:23.142589Z",
     "iopub.status.busy": "2022-08-01T12:09:23.141807Z",
     "iopub.status.idle": "2022-08-01T12:09:23.181580Z",
     "shell.execute_reply": "2022-08-01T12:09:23.180734Z"
    },
    "papermill": {
     "duration": 0.053912,
     "end_time": "2022-08-01T12:09:23.183616",
     "exception": false,
     "start_time": "2022-08-01T12:09:23.129704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = TransactionsRnn(transaction_features, embedding_projections).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "333e6a41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:23.208066Z",
     "iopub.status.busy": "2022-08-01T12:09:23.207767Z",
     "iopub.status.idle": "2022-08-01T12:09:26.933466Z",
     "shell.execute_reply": "2022-08-01T12:09:26.932467Z"
    },
    "papermill": {
     "duration": 3.740999,
     "end_time": "2022-08-01T12:09:26.936115",
     "exception": false,
     "start_time": "2022-08-01T12:09:23.195116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BiGRU2FCL(transaction_features, embedding_projections).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b538a27a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:26.961244Z",
     "iopub.status.busy": "2022-08-01T12:09:26.960928Z",
     "iopub.status.idle": "2022-08-01T12:09:27.003460Z",
     "shell.execute_reply": "2022-08-01T12:09:27.002411Z"
    },
    "papermill": {
     "duration": 0.057776,
     "end_time": "2022-08-01T12:09:27.005775",
     "exception": false,
     "start_time": "2022-08-01T12:09:26.947999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiGRU2FCL(\n",
       "  (_transaction_cat_embeddings): ModuleList(\n",
       "    (0): Embedding(12, 6, padding_idx=0)\n",
       "    (1): Embedding(8, 5, padding_idx=0)\n",
       "    (2): Embedding(176, 29, padding_idx=0)\n",
       "    (3): Embedding(23, 9, padding_idx=0)\n",
       "    (4): Embedding(5, 3, padding_idx=0)\n",
       "    (5): Embedding(4, 3, padding_idx=0)\n",
       "    (6): Embedding(8, 5, padding_idx=0)\n",
       "    (7): Embedding(4, 3, padding_idx=0)\n",
       "    (8): Embedding(109, 22, padding_idx=0)\n",
       "    (9): Embedding(25, 9, padding_idx=0)\n",
       "    (10): Embedding(164, 28, padding_idx=0)\n",
       "    (11): Embedding(29, 10, padding_idx=0)\n",
       "    (12): Embedding(8, 5, padding_idx=0)\n",
       "    (13): Embedding(25, 9, padding_idx=0)\n",
       "    (14): Embedding(54, 15, padding_idx=0)\n",
       "    (15): Embedding(11, 6, padding_idx=0)\n",
       "    (16): Embedding(24, 9, padding_idx=0)\n",
       "    (17): Embedding(11, 6, padding_idx=0)\n",
       "  )\n",
       "  (_product_embedding): Embedding(6, 4)\n",
       "  (_gru): GRU(182, 128, batch_first=True, bidirectional=True)\n",
       "  (_top_classifier): Linear(in_features=260, out_features=64, bias=True)\n",
       "  (_intermediate_activation): ReLU()\n",
       "  (_head): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "724b712e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:27.030386Z",
     "iopub.status.busy": "2022-08-01T12:09:27.030073Z",
     "iopub.status.idle": "2022-08-01T12:09:27.070811Z",
     "shell.execute_reply": "2022-08-01T12:09:27.069981Z"
    },
    "papermill": {
     "duration": 0.055518,
     "end_time": "2022-08-01T12:09:27.072830",
     "exception": false,
     "start_time": "2022-08-01T12:09:27.017312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lr=1e-3, params=model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7281fb",
   "metadata": {
    "papermill": {
     "duration": 0.011151,
     "end_time": "2022-08-01T12:09:27.095748",
     "exception": false,
     "start_time": "2022-08-01T12:09:27.084597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* Запустим цикл обучения, каждую эпоху будем логировать лосс, а так же roc-auc на валидации и на обучении. Будем сохрнаять веса после каждой эпохи, а так же лучшие с помощью early_stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42349265",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T12:09:27.120448Z",
     "iopub.status.busy": "2022-08-01T12:09:27.119543Z",
     "iopub.status.idle": "2022-08-01T12:57:26.542742Z",
     "shell.execute_reply": "2022-08-01T12:57:26.539415Z"
    },
    "papermill": {
     "duration": 2879.44133,
     "end_time": "2022-08-01T12:57:26.548520",
     "exception": false,
     "start_time": "2022-08-01T12:09:27.107190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 505it [00:34, 29.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 500 batches: 0.11998060345649719\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 1003it [01:07, 15.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 1000 batches: 0.11451825499534607\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 1503it [01:35, 39.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 1500 batches: 0.11094451695680618\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 2001it [02:09, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 2000 batches: 0.10820449888706207\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 2500it [02:37, 15.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 2500 batches: 0.1104799136519432\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 3001it [03:11, 25.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 3000 batches: 0.11117072403430939\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 3502it [03:38, 30.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 3500 batches: 0.11109358817338943\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 4000it [04:11, 32.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 4000 batches: 0.11129948496818542\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 4508it [04:44, 16.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 4500 batches: 0.11034287512302399\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 5005it [05:14, 32.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 5000 batches: 0.10946216434240341\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 5062it [05:16, 16.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after epoch: 0.10936491191387177\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 5033it [01:12, 69.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC-AUC improved (-inf --> 0.759229).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 5062it [02:52, 29.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Train roc-auc: 0.7658652414934245, Val roc-auc: 0.7592293970645178\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 503it [00:25, 23.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 500 batches: 0.09802482277154922\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 1004it [00:53, 22.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 1000 batches: 0.10299616307020187\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 1502it [01:18, 29.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 1500 batches: 0.10196655243635178\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 2006it [01:50, 24.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 2000 batches: 0.10326703637838364\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 2503it [02:24, 16.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 2500 batches: 0.10447956621646881\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 3000it [02:52, 16.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 3000 batches: 0.10533963888883591\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 3504it [03:25, 14.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 3500 batches: 0.10464069992303848\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 4005it [03:54, 26.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 4000 batches: 0.10411983728408813\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 4504it [04:26, 17.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 4500 batches: 0.1043473556637764\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 5003it [04:57, 29.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 5000 batches: 0.10321163386106491\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 5062it [04:59, 16.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after epoch: 0.1029847264289856\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 5033it [01:12, 69.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC-AUC improved (0.759229 --> 0.764997).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 5062it [02:27, 34.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Train roc-auc: 0.7871248689119836, Val roc-auc: 0.7649966820181826\n",
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 508it [00:26, 32.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 500 batches: 0.112856425344944\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 1014it [00:53, 32.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 1000 batches: 0.10378293693065643\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 1504it [01:18, 28.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 1500 batches: 0.1015501469373703\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 2005it [01:45, 26.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 2000 batches: 0.09950921684503555\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 2500it [02:12, 34.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 2500 batches: 0.09950658679008484\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 3004it [02:41, 24.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 3000 batches: 0.0997152179479599\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 3504it [03:11,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 3500 batches: 0.10043374449014664\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 4003it [03:35, 26.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 4000 batches: 0.09976272284984589\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 4505it [04:09, 23.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 4500 batches: 0.09907898306846619\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 5010it [04:40, 40.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 5000 batches: 0.09899305552244186\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 5062it [04:42, 17.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after epoch: 0.0988345816731453\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 5033it [01:14, 67.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC-AUC improved (0.764997 --> 0.766579).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 5062it [02:09, 39.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Train roc-auc: 0.8173900349506174, Val roc-auc: 0.7665791484326362\n",
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 504it [00:25, 25.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 500 batches: 0.08541794866323471\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 1004it [00:53,  4.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 1000 batches: 0.09171485155820847\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 1503it [01:22, 23.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 1500 batches: 0.09199900925159454\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 2004it [01:48, 23.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 2000 batches: 0.0957365557551384\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 2504it [02:13, 25.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 2500 batches: 0.09338313341140747\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 3007it [02:43, 37.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 3000 batches: 0.09232534468173981\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 3503it [03:18, 12.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 3500 batches: 0.09313011169433594\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 4003it [03:47, 28.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 4000 batches: 0.09337907284498215\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 4504it [04:15, 22.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 4500 batches: 0.09422532469034195\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 5001it [04:43, 26.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 5000 batches: 0.09404565393924713\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 5062it [04:46, 17.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after epoch: 0.09397752583026886\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 5033it [01:13, 68.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No imporvement in Validation ROC-AUC. Current: 0.760248. Current best: 0.766579\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 5062it [02:02, 41.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed. Train roc-auc: 0.8441058079244194, Val roc-auc: 0.7602479946202771\n",
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 503it [00:32, 18.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 500 batches: 0.08141770213842392\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 1002it [01:01, 19.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 1000 batches: 0.08098305016756058\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 1502it [01:26, 21.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 1500 batches: 0.08243240416049957\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 2008it [01:55, 24.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 2000 batches: 0.08178344368934631\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 2503it [02:17, 40.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 2500 batches: 0.08227613568305969\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 3009it [02:43, 30.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 3000 batches: 0.08329416066408157\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 3504it [03:12, 20.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 3500 batches: 0.08372504264116287\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 4004it [03:45, 27.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 4000 batches: 0.083635114133358\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 4500it [04:18, 17.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 4500 batches: 0.08417848497629166\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 5009it [04:46, 38.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 5000 batches: 0.083115354180336\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 5062it [04:48, 17.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after epoch: 0.0829308032989502\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 5033it [01:12, 69.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No imporvement in Validation ROC-AUC. Current: 0.749861. Current best: 0.766579\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 5062it [02:01, 41.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed. Train roc-auc: 0.8882123506198016, Val roc-auc: 0.7498606084924796\n",
      "Starting epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 503it [00:25, 25.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 500 batches: 0.06793341785669327\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 1004it [00:52, 22.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 1000 batches: 0.07385522872209549\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 1508it [01:16, 29.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 1500 batches: 0.07187753915786743\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 2002it [01:44, 20.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 2000 batches: 0.07006820291280746\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 2508it [02:11, 27.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 2500 batches: 0.06999258697032928\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 3002it [02:36, 21.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 3000 batches: 0.0709347054362297\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 3505it [03:03, 21.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 3500 batches: 0.07222140580415726\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 4003it [03:29, 19.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 4000 batches: 0.07247376441955566\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 4502it [04:03, 22.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 4500 batches: 0.07300768792629242\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 5003it [04:30, 19.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 5000 batches: 0.07268665730953217\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 5062it [04:32, 18.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after epoch: 0.0726662203669548\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 5033it [01:12, 69.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No imporvement in Validation ROC-AUC. Current: 0.729631. Current best: 0.766579\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping reached. Stop training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    train_epoch(model, optimizer, dataset_train, batch_size=train_batch_size, \n",
    "                shuffle=True, print_loss_every_n_batches=500, device=device)\n",
    "    \n",
    "    val_roc_auc = eval_model(model, dataset_val, batch_size=val_batch_szie, device=device)\n",
    "    es(val_roc_auc, model)\n",
    "    scheduler.step(val_roc_auc)\n",
    "    \n",
    "    if es.early_stop:\n",
    "        print('Early stopping reached. Stop training...')\n",
    "        break\n",
    "    torch.save(model.state_dict(), os.path.join(path_to_checkpoints, f'epoch_{epoch+1}_val_{val_roc_auc:.3f}.pt'))\n",
    "    \n",
    "    train_roc_auc = eval_model(model, dataset_train, batch_size=val_batch_szie, device=device)\n",
    "    print(f'Epoch {epoch+1} completed. Train roc-auc: {train_roc_auc}, Val roc-auc: {val_roc_auc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2904.013655,
   "end_time": "2022-08-01T12:57:30.774444",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-01T12:09:06.760789",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
